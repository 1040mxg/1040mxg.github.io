{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5334proj",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7MkoXO3eVjmYANieKViKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1040mxg/1040mxg.github.io/blob/master/5334proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMYGyMoit6up"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKGTfUbLt92h"
      },
      "source": [
        "[Blog homepage](https://1040mxg.github.io/blog/) || [View on Github](https://github.com/1040mxg/5334project)\n",
        "\n",
        "This project will build a classifier to detect specific media bias, and find the probability that a given piece of written news is by, or influenced by, Chinese state-sponsored media.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od6CHvb4OFJ_"
      },
      "source": [
        "# Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umFf9OGOOGon"
      },
      "source": [
        "This project makes use of the Natural Language Toolkit (NLTK) to help process text. I will be using **Naive Bayes** as a base to build this classifier. Naive Bayes is a simple, fast, and accurate algorithm that works particularly well with natural language processing (NLP), or text classification. In my tests, I used the **Multinomial Naive Bayes** algorithm. It works by taking advantage of probability theory and Bayes' Theorem to classify text. For each piece of input text, the probability of each possible class is calculated and the final classification made based on highest probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkctoBT4O7lJ"
      },
      "source": [
        "I will first run the Naive Bayes classifier (NBC) through the datasets separated by topic, then all as one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAnhYFeqNXi4"
      },
      "source": [
        "pip install --user -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcx64r1eoMK"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5n-lDNHewYU"
      },
      "source": [
        "First, I import the desired files and load necessary libraries. Here, I've imported the NLTK corpus of English-language stopwords (common articles and prepositions such as \"the\", \"an\", \"there\" that are largely irrelevant in data mining)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DQQlUxPRIJO",
        "outputId": "fb3283e3-78d9-4c52-fa72-36d1a87efbd9"
      },
      "source": [
        "###import necessary libaries\n",
        "import nltk\n",
        "from nltk.corpus import words, stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "words = words.words()"
      ],
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISIT8rhzwd00"
      },
      "source": [
        "I combine all the data from each source into one here, but the data can also be analysed by topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MSg-vlsv58D"
      },
      "source": [
        "filenames1 = ['tweets_xh_hk.txt', 'tweets_xh_taiwan.txt', 'tweets_xh_xinjiang.txt']\n",
        "filenames2 = ['tweets_npr_hk.txt', 'tweets_npr_tw.txt', 'tweets_npr_xinjiang.txt']\n",
        "\n",
        "with open('tweets_xh_all.txt', 'w') as outfile:\n",
        "    for names in filenames1:\n",
        "        with open(names) as infile:\n",
        "            outfile.write(infile.read())\n",
        "\n",
        "with open('tweets_npr_all.txt', 'w') as outfile:\n",
        "    for names in filenames2:\n",
        "        with open(names) as infile:\n",
        "            outfile.write(infile.read())\n",
        "\n",
        "X = []\n",
        "y = []"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmhNNz6CRKLO"
      },
      "source": [
        "Then I load in the data from NPR and Xinhua News, label them, and combine them into one dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVyQhmWixBWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0019203f-44e5-49e8-9a2c-a9813cfb5c0a"
      },
      "source": [
        "##import data files and print some basic stats\n",
        "###data files will default to combined data unless specified\n",
        "\n",
        "def loadData(file1 = \"tweets_xh_all.txt\", file2=\"tweets_npr_all.txt\"):\n",
        "#first, load the XH and NPR data individually\n",
        "  xhData = []\n",
        "  nprData = []\n",
        "  with open(file1, 'r') as f:\n",
        "    for row in f:\n",
        "      xhData.append(row+'0')\n",
        "  with open(file2, 'r') as f:\n",
        "    for row in f:\n",
        "      nprData.append(row+'1')\n",
        "  print(\"XH items:\", len(xhData))\n",
        "  print(\"NPR items:\", len(nprData))\n",
        "\n",
        "\n",
        "#compile the NPR and XH files into one\n",
        "  data = []\n",
        "  for i in range(len(xhData)):\n",
        "    if len(xhData[i]) > 25:\n",
        "      data.append([xhData[i], '0'])\n",
        "  for i in range(len(nprData)):\n",
        "    if len(nprData[i]) > 25:\n",
        "      data.append([nprData[i], '1'])\n",
        "  print(\"Total data items:\", len(data))\n",
        "  return data\n",
        "\n",
        "data = loadData()"
      ],
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XH items: 6299\n",
            "NPR items: 698\n",
            "Total data items: 6710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3PbO1BNSSQR"
      },
      "source": [
        "Next, I shuffle the data and split it into training, dev, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzyTQC5l_LZ2",
        "outputId": "32df078b-a7fe-4003-a7c6-ab5ebf85416e"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "random.shuffle(data)\n",
        "dataLen = len(data)\n",
        "print(dataLen)\n",
        "\n",
        "###manual division\n",
        "testLen = int(.2*dataLen)\n",
        "devLen = int(.2*(dataLen - testLen))\n",
        "trainLen = dataLen - testLen - devLen\n",
        "\n",
        "train = []\n",
        "dev = []\n",
        "test =[]\n",
        "for i in range(trainLen):\n",
        "  train.append(data[i])\n",
        "\n",
        "for i in range(trainLen, trainLen+devLen):\n",
        "  dev.append(data[i])\n",
        "\n",
        "for i in range(dataLen-testLen, dataLen):\n",
        "  test.append(data[i])\n",
        "\n",
        "print(\"Train Data: %d items\" %(len(train)))\n",
        "print(\"Dev Data: %d items\" %(len(dev)))\n",
        "print(\"Test Data: %d items\" %(len(test)))"
      ],
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6710\n",
            "Train Data: 4295 items\n",
            "Dev Data: 1073 items\n",
            "Test Data: 1342 items\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Lh-J39H3lS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ1Tj1tBe4az"
      },
      "source": [
        "The training set is split into two lists based on source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Ljw_8QEvy1",
        "outputId": "bc7ed7f6-0831-43be-cf48-293a3bf1c8f5"
      },
      "source": [
        "def split(data):\n",
        "  nprStr = []\n",
        "  xhStr = []\n",
        "  for i in range(len(data)):\n",
        "    if data[i][-1] == '1':\n",
        "      nprStr.append(data[i][0])\n",
        "    elif data[i][-1] == '0':\n",
        "      xhStr.append(data[i][0])\n",
        "  return nprStr, xhStr\n",
        "\n",
        "def printSample(nprStr, xhStr):\n",
        "  print(\"Xinhua Tweets: %d items\" %(len(xhStr)))\n",
        "  print(\"---------------Sample---------------\")\n",
        "  for i in range(5):\n",
        "    if(len(xhStr[i])>80):\n",
        "      print(\"%s...\" %xhStr[i][0:80])\n",
        "    else:\n",
        "      print(xhStr[i], end='')\n",
        "  print(\"\\n\")\n",
        "  print(\"NPR Tweets: %d items\" %(len(nprStr)))\n",
        "  print(\"---------------Sample---------------\")\n",
        "  for i in range(5):\n",
        "    if(len(nprStr[i])>80):\n",
        "      print(\"%s...\" %nprStr[i][0:80])\n",
        "    else:\n",
        "      print(nprStr[i], end='')\n",
        "\n",
        "nprStr, xhStr = split(train)\n",
        "printSample(nprStr, xhStr)"
      ],
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xinhua Tweets: 3985 items\n",
            "---------------Sample---------------\n",
            "Ethnic groups in Xinjiang are part of Chinese nation: white paper xhne.ws/znLOe ...\n",
            "3. Panda recognition tech xhne.ws/oKuA9 https://t.co/gjVitH2EFX\"\n",
            "0Hong Kong immigration director vows to attract more talents xhne.ws/Zs4nP (Xinhu...\n",
            "Findings at the caves, dubbed \"gateway to heaven\", will fill the gap of Xinjiang...\n",
            "300-km-long desert road, crossing China's largest desert Taklimakan nicknamed \"S...\n",
            "\n",
            "\n",
            "NPR Tweets: 310 items\n",
            "---------------Sample---------------\n",
            "The association has been ordered to pay the equivalent of $15,000 and was given ...\n",
            "A Hong Kong district court has found nine activists guilty of public nuisance cr...\n",
            "Hong Kong has 110 McDonalds outlets that are open 24 hours, and they are relucta...\n",
            "Beijing Brushes Aside Hong Kongs Rejection Of Electoral Reform n.pr/1Bm1nz0\n",
            "1After bringing the number of local coronavirus cases down to zero, Hong Kong is ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs-ckUGRJ4Ls"
      },
      "source": [
        "The data lists are further processed into dictionaries of vocabularies. \n",
        "Each line in the lists was split into individual words and added to a temporary dictionary. This temporary dictionary was then trimmed to include only words that do not appear in the list of stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnnuiVkUK3DV",
        "outputId": "9bf82596-08f2-46ff-d472-bcb78eadafbd"
      },
      "source": [
        "import re\n",
        "\n",
        "#trainset vocab list + split into npr/xh\n",
        "def splitWord(strings, tempDict):\n",
        "  words = 0\n",
        "  for line in strings:\n",
        "    line = line.lower()\n",
        "    temp = re.findall(r'\\w+', line)\n",
        "    for word in temp:\n",
        "      words+=1\n",
        "      if word in tempDict:\n",
        "        tempDict[word] +=1\n",
        "      else:\n",
        "        tempDict[word] = 1\n",
        "\n",
        "def trim(strings, tempDict):\n",
        "  keys = list(tempDict.keys())\n",
        "  vals = list(tempDict.values())\n",
        "  newDict = dict()\n",
        "  count = 0\n",
        "  for i in range(len(vals)):\n",
        "    k = keys[i]\n",
        "    if k not in stopwords and k in words:\n",
        "      newDict[k] = vals[i]\n",
        "      count+=vals[i]\n",
        "  return newDict, count\n",
        "\n",
        "def makeVocab(strings):\n",
        "  tempDict = dict()\n",
        "  splitWord(strings, tempDict)\n",
        "  newDict, count = trim(strings, tempDict)\n",
        "  return newDict, count\n",
        "\n",
        "print(\"---------------Xinhua Word List---------------\")\n",
        "xhVocab, xhCount = makeVocab(xhStr)\n",
        "print(\"Words After Trimming: \", xhCount)\n",
        "print(\"Unique Words: %d\\n\" %len(xhVocab))\n",
        "\n",
        "print(\"---------------NPR Word List---------------\")\n",
        "nprVocab, nprCount = makeVocab(nprStr)\n",
        "print(\"Words After Trimming: \", nprCount)\n",
        "print(\"Unique Words: %d\\n\" %len(nprVocab))\n",
        "\n",
        "print(\"---------------All Words---------------\")\n",
        "allVocab = {**nprVocab, **xhVocab}\n",
        "allCount = nprCount+xhCount\n",
        "print(\"Total Word Count: \", allCount)\n",
        "print(\"Unique Words: \", len(allVocab))\n",
        "\n",
        "# print(\"Positive:\", nprVocab)\n",
        "# print(\"Negative:\", xhVocab)\n",
        "# print(\"All:\", allVocab)  "
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Xinhua Word List---------------\n",
            "Words After Trimming:  32761\n",
            "Unique Words: 4532\n",
            "\n",
            "---------------NPR Word List---------------\n",
            "Words After Trimming:  2541\n",
            "Unique Words: 1046\n",
            "\n",
            "---------------All Words---------------\n",
            "Total Word Count:  35302\n",
            "Unique Words:  4759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IXl4rFhBcS6"
      },
      "source": [
        "P(xh) and P(npr) were calculated by dividing the count of each by the total (trimmed) word count.\n",
        "\n",
        "Individual P(word|author) were calculated using (#word in class)/(#total word count in sentiment class). A general P(word) was also calculated using (#word/#total word count). All three were stored in new dictionaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN6f3qdokziO"
      },
      "source": [
        "In addition, one major issue that Naive Bayes faces in text classification is missing data. When a word does not appear in a class, making P(word|class) = 0, that probability gets multiplied out to all the other probabilities in determining P(class), making P(class) = 0.\n",
        "\n",
        "Laplace smoothing is a method of combatting this issue. The process is:\n",
        "1.   Add 1 to every count so P(word|class) will always > 0.\n",
        "2.   Balance this by adding the number of possible words to the divisor so the result will never < 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Jx9GQf_pfE",
        "outputId": "4e8aa35a-295d-4282-dee5-b19e9548cdcd"
      },
      "source": [
        "#get probabilities for P(npr), P(xh) based on trimmed word counts\n",
        "allCount = nprCount+xhCount\n",
        "print(\"---------------Probabilities---------------\")\n",
        "print(\"P(npr) = %d/%d = %.5f\" %(nprCount, allCount, nprCount/allCount))\n",
        "print(\"P(xh) = %d/%d = %.5f\\n\" %(xhCount, allCount, xhCount/allCount))\n",
        "\n",
        "##laplace smoothing\n",
        "nprCount+=len(allVocab)\n",
        "xhCount+=len(allVocab)\n",
        "allCount = nprCount+xhCount\n",
        "#for test words of P=0\n",
        "noNPR = 1/nprCount\n",
        "noXH = 1/xhCount\n",
        "\n",
        "#get individual P(word|class) and store in dictionary\n",
        "def getProbs(Dict, PCount):\n",
        "  keys = list(Dict.keys())\n",
        "  vals = list(Dict.values())\n",
        "  newDict = dict()\n",
        "  for i in range(len(vals)):\n",
        "    k = keys[i]\n",
        "    p = vals[i]/PCount\n",
        "    newDict[k] = p\n",
        "  return newDict\n",
        "\n",
        "nprProb = getProbs(nprVocab, nprCount)\n",
        "xhProb = getProbs(xhVocab, xhCount)\n",
        "allProb = getProbs(allVocab, allCount)\n",
        "\n",
        "print(\"---------------P(word) Sample---------------\")\n",
        "print({k: allProb[k] for k in list(allProb)[:5]}, \"\\n\")\n",
        "\n",
        "print(\"---------------P(word|npr) Sample---------------\")\n",
        "print({k: nprProb[k] for k in list(nprProb)[:5]}, \"\\n\")\n",
        "\n",
        "print(\"---------------P(word|xh) Sample---------------\")\n",
        "print({k: xhProb[k] for k in list(xhProb)[:5]})"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Probabilities---------------\n",
            "P(npr) = 2541/35302 = 0.07198\n",
            "P(xh) = 32761/35302 = 0.92802\n",
            "\n",
            "---------------P(word) Sample---------------\n",
            "{'association': 0.00015618027666220436, 'ordered': 2.2311468094600625e-05, 'pay': 2.2311468094600625e-05, 'equivalent': 2.2311468094600625e-05, 'given': 0.00015618027666220436} \n",
            "\n",
            "---------------P(word|npr) Sample---------------\n",
            "{'association': 0.000273972602739726, 'ordered': 0.000136986301369863, 'pay': 0.000136986301369863, 'equivalent': 0.000136986301369863, 'given': 0.00041095890410958907} \n",
            "\n",
            "---------------P(word|xh) Sample---------------\n",
            "{'ethnic': 0.0010394456289978678, 'part': 0.0008528784648187633, 'nation': 0.0002398720682302772, 'white': 0.0013326226012793177, 'paper': 0.0012260127931769723}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RATcYep5RsAj"
      },
      "source": [
        "# Testing Model with Dev Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4pgHOKOxT7s"
      },
      "source": [
        "First, just applying the algorithm directing to the dev set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQIA7MWhbNDc",
        "outputId": "21df10fe-b29c-4080-e6ce-048af99f4167"
      },
      "source": [
        "def splitTrim(line):\n",
        "  words = []\n",
        "  temp = re.findall(r'\\w+', line)\n",
        "  for i in temp:\n",
        "    if len(i)>2:\n",
        "      words.append(i)\n",
        "  #print(words)\n",
        "  return words\n",
        "\n",
        "#with Laplace smoothing\n",
        "def naiveBayes(line, vocab0, vocab1):\n",
        "  words = splitTrim(line)\n",
        "  p_npr = 1\n",
        "  p_xh = 1\n",
        "  for word in words:\n",
        "    if word in vocab1:\n",
        "      p_npr = nprProb[word]*p_npr\n",
        "    else:\n",
        "      p_npr = p_npr*noNPR\n",
        "    if word in vocab0:\n",
        "      p_xh = xhProb[word]*p_xh\n",
        "    else:\n",
        "      p_xh = p_xh*noXH\n",
        "  if p_npr < p_xh:\n",
        "    pred = 0\n",
        "  elif p_npr > p_xh:\n",
        "    pred = 1\n",
        "  else:\n",
        "    return 2\n",
        "  return pred\n",
        "\n",
        "def predict(data, algorithm=naiveBayes, vocab0=xhVocab, vocab1=nprVocab):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for line in data:\n",
        "    actual = int(line[-1])\n",
        "    pred = algorithm(line[0], vocab0, vocab1)\n",
        "    #print(\"Test: \",pred, actual)\n",
        "    if pred == actual:\n",
        "      correct+=1\n",
        "      total+=1\n",
        "    else:\n",
        "      total+=1\n",
        "  acc = correct/total\n",
        "  return acc, correct, total\n",
        "\n",
        "acc, correct, total = predict(dev)\n",
        "acc = acc*100\n",
        "print(\"Accuracy: %.3f \" %acc, \"%\")\n",
        "\n",
        "X.append(\"Dev\")\n",
        "y.append(acc)"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 9.413  %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JNu39gE0p_6"
      },
      "source": [
        "**Five-Fold Cross Validation**\n",
        "\n",
        "This tests the algorithm using cross validation. The process is as follows:\n",
        "1.   Split the data into 5 groups\n",
        "2.   For each group, use it as a test set with the remaining groups as training sets\n",
        "3.   Fit a model on the training set and evaluate on the test set\n",
        "4.   Repeat the above until all samples have had a turn as a test set\n",
        "5.   Find the mean accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iei0O6RSdMG6",
        "outputId": "80db3220-ba79-4ac0-948f-4aa87f85cc9e"
      },
      "source": [
        "#split dataset into 5 folds\n",
        "def crossValsplit(dataset):\n",
        "  folds = 5\n",
        "  data_split = list()\n",
        "  data_copy = list(dataset)\n",
        "  fold_size = int(len(dataset)/folds)\n",
        "  for i in range(folds):\n",
        "    fold = list()\n",
        "    while len(fold) < fold_size:\n",
        "      index = randrange(len(data_copy))\n",
        "      fold.append(data_copy.pop(index))\n",
        "    data_split.append(fold)\n",
        "  return data_split\n",
        "\n",
        "def evaluate(data):\n",
        "  folds = crossValsplit(data)\n",
        "  scores = list()\n",
        "  for fold in folds:\n",
        "    train = list(folds)\n",
        "    train.remove(fold)\n",
        "    train = sum(train, [])\n",
        "    test = list()\n",
        "    for row in fold:\n",
        "      test.append(row)\n",
        "    #create new dictionaries based on fold\n",
        "    x_nprStr, x_xhStr = split(train)\n",
        "    x_nprVocab, x_nprCount = makeVocab(x_nprStr)\n",
        "    x_xhVocab, x_xhCount = makeVocab(x_xhStr)\n",
        "    x_allVocab = {**x_nprVocab, **x_xhVocab}\n",
        "    x_allCount = x_nprCount+x_xhCount\n",
        "    x_nprProb = getProbs(x_nprVocab, x_nprCount)\n",
        "    x_xhProb = getProbs(x_xhVocab, x_xhCount)\n",
        "    x_allProb = getProbs(x_allVocab, x_allCount)\n",
        "    acc, correct, total = predict(test)\n",
        "    acc = acc*100\n",
        "    scores.append(acc)\n",
        "  print(\"Scores: %s\" %scores)\n",
        "  mean = sum(scores)/float(len(scores))\n",
        "  print('Mean Accuracy: %.3f%%' %mean)\n",
        "    \n",
        "mean = evaluate(dev)\n",
        "\n",
        "X.append(\"X-valid\")\n",
        "y.append(mean)"
      ],
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [9.813084112149532, 7.476635514018691, 14.018691588785046, 7.943925233644859, 7.943925233644859]\n",
            "Mean Accuracy: 9.439%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_T-sz805d3n"
      },
      "source": [
        "Even with Laplace smoothing, the algorithm is not very accurate, on either the cross-validation or applied directly to the dev data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOgzdecE6ZB"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwQgG6ct5s9p"
      },
      "source": [
        "I will start approaching this problem from a few different directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWFHBiBE5wQv"
      },
      "source": [
        "First, instead of a pure \"yes/no\" comparison, I want to see how the classifier and training data do in predicting P(class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LHhJLe6eqw"
      },
      "source": [
        "Loading a new dataset of only Xinhua data and leaving it unlabelled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzN2rCZT6N2W",
        "outputId": "17a7350c-76ca-4b77-dd99-2cd2e1351d5b"
      },
      "source": [
        "newData = []\n",
        "\n",
        "with open(\"tweets_xh_hk.txt\", 'r') as f:\n",
        "    for row in f:\n",
        "      newData.append(row)\n",
        "\n",
        "for i in range(5):\n",
        "  print(newData[i][0:80], \"...\")"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 2020 policy address delivered by HKSAR Chief Executive Carrie Lam has won st ...\n",
            "The national security law in China's Hong Kong has been remarkably effective in  ...\n",
            "Hong Kong embraces greater development opportunities with new policy address unv ...\n",
            "HKSAR Chief Executive Carrie Lam delivered the 2020 policy address, highlighting ...\n",
            "The central authorities will fully support Hong Kong's development in 7 aspects  ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWb7r9CGMXAi"
      },
      "source": [
        "Adjusting the Naive Bayes method to return P(class) instead of a prediction and taking the mean probability of each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaZGNKJ37-Ju",
        "outputId": "5e8afc1e-b29e-4326-f9e1-3ade10129ca2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#with Laplace smoothing\n",
        "def naiveBayesPoss(line, vocab0, vocab1):\n",
        "  words = splitTrim(line)\n",
        "  #print(words)\n",
        "  p_npr = 1\n",
        "  p_xh = 1\n",
        "  for word in words:\n",
        "    if word in vocab1:\n",
        "      p_npr = nprProb[word]*p_npr\n",
        "    else:\n",
        "      p_npr = p_npr*noNPR\n",
        "    if word in vocab0:\n",
        "      p_xh = xhProb[word]*p_xh\n",
        "    else:\n",
        "      p_xh = p_xh*noXH\n",
        "  return p_npr, p_xh\n",
        "\n",
        "def predictPoss(data, vocab0=xhVocab, vocab1=nprVocab):\n",
        "  arrNPR = []\n",
        "  arrXH = []\n",
        "  for line in data:\n",
        "    #print(line)\n",
        "    predNPR, predXH = naiveBayesPoss(line, vocab0, vocab1)\n",
        "    arrNPR.append(predNPR)\n",
        "    arrXH.append(predXH)\n",
        "    #print(\"Test: \",pred, actual)\n",
        "  possNPR = np.mean(arrNPR)\n",
        "  possXH = np.mean(arrXH)\n",
        "  return possNPR, possXH\n",
        "\n",
        "possNPR, possXH = predictPoss(newData)\n",
        "print('Mean possXH: %.3f%%' %possXH)\n",
        "print('Mean possNPR: %.3f%%' %possNPR)\n",
        "\n",
        "X.append(\"Mean\\nP(class)\")\n",
        "y.append(possXH)"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean possXH: 0.002%\n",
            "Mean possNPR: 0.002%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMFWNWMMLr_6"
      },
      "source": [
        "Given that the data used in the experiment above *only* came from Xinhua, the mean P(class) should have been much higher than it is, or at least higher than possNPR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPSZb_q_OBO7"
      },
      "source": [
        "**Top Words**\n",
        "\n",
        "Next, I will derive the top 10 words for predicting class, ie. the top 10 each of P(Positive|word) and P(Negative|word). Given that:\n",
        "\n",
        "$P(class|word)=\\frac{P(word|class)P(class)}{P(word)}$\n",
        "\n",
        "and I already have the values for P(word), P(class), and P(word|class), this will be a simple matter of finding the top 10 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Odg-WUod2G",
        "outputId": "2b8d382e-ab2b-4e7c-affa-8c69d889f274"
      },
      "source": [
        "\"\"\"\n",
        "Find the top ten P(class|word) for each class\n",
        "\"\"\"\n",
        "def topTen(classDict, classCount):\n",
        "  words = []\n",
        "  probs = []\n",
        "  #keys = word, vals = P(word|class)\n",
        "  keys = list(classDict.keys())\n",
        "  vals = list(classDict.values())\n",
        "  pClass = classCount/allCount\n",
        "  for i in range(len(keys)):\n",
        "    pWord = allProb[(keys[i])]\n",
        "    p = (vals[i]*pClass)/pWord\n",
        "    words.append(keys[i])\n",
        "    probs.append(p)\n",
        "  top = sorted(range(len(probs)), key=lambda i: probs[i])[-10:]\n",
        "  topWords = []\n",
        "  for i in top:\n",
        "    topWords.append(words[i])\n",
        "  return topWords\n",
        "\n",
        "topNPR = topTen(nprProb, nprCount)\n",
        "topXH = topTen(xhProb, xhCount)\n",
        "\n",
        "print(\"Top NPR Words:\")\n",
        "print(topNPR)\n",
        "print(\"\\nTop XH Words:\")\n",
        "print(topXH)\n",
        "\n",
        "acc, correct, total = predict(data, algorithm=naiveBayes, vocab0=topXH, vocab1=topNPR)\n",
        "acc = acc*100\n",
        "print(\"\\nAccuracy, Top10 Words: %.3f%%\" %acc)\n",
        "\n",
        "X.append(\"Top10\")\n",
        "y.append(acc)"
      ],
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top NPR Words:\n",
            "['whose', 'enemy', 'withdraw', 'remarkably', 'shut', 'strike', 'tear', 'extradition', 'pro', 'n']\n",
            "\n",
            "Top XH Words:\n",
            "['struck', 'heart', 'migrant', 'properly', 'policeman', 'afternoon', 'bloom', 'therapy', 'low', 'battle']\n",
            "\n",
            "Accuracy, Top10 Words: 7.481%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUJQHAdug4L"
      },
      "source": [
        "Changing the parameters to top 10 words only decreased accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "eTuVpwl7OBny",
        "outputId": "6a141bbe-81b4-436b-9b9f-038a2d108dc1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y[1] = 9.439\n",
        "\n",
        "plt.title(\"Prediction Accuracy\")\n",
        "plt.ylim(0,100)\n",
        "plt.xlabel(\"Method\")\n",
        "plt.ylabel(\"%Accuracy\")\n",
        "plt.bar(X,y)\n",
        "plt.show()"
      ],
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfFAfxGncs6s"
      },
      "source": [
        "None of these methods are very strong. Clearly, this dataset is very different from something like a dataset of movie reviews. Even though the general idea is similar (movie reviews using \"positive\" and \"negative\" words to find sentiment), "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYg5bqyjlYLS"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvedlFEeXnk"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEBB5Z5GeZyj"
      },
      "source": [
        "*   Bruno Stecanella, \"A practical explanation of a Naive Bayes classifier\", https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n",
        "*   Jason Brownlee, \"Naive Bayes Classifier From Scratch in Python\n",
        "\", https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
        "\n",
        "\n"
      ]
    }
  ]
}